{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146182\n",
      "49157\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "요\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n",
      "pad_sequences maxlen :  47\n",
      "전체 문장의 95.2175448835102%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + round(2.5 * np.std(num_tokens))\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,117,377\n",
      "Trainable params: 1,117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 8  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 학습을 조기 종료(Early Stopping)합니다. 또한, ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1945/1950 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8110\n",
      "Epoch 00001: val_acc improved from -inf to 0.85122, saving model to best_model.h5\n",
      "1950/1950 [==============================] - 13s 7ms/step - loss: 0.4161 - acc: 0.8111 - val_loss: 0.3513 - val_acc: 0.8512\n",
      "Epoch 2/15\n",
      "1942/1950 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8603\n",
      "Epoch 00002: val_acc improved from 0.85122 to 0.86045, saving model to best_model.h5\n",
      "1950/1950 [==============================] - 11s 6ms/step - loss: 0.3308 - acc: 0.8603 - val_loss: 0.3565 - val_acc: 0.8605\n",
      "Epoch 3/15\n",
      "1944/1950 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.8715\n",
      "Epoch 00003: val_acc improved from 0.86045 to 0.86476, saving model to best_model.h5\n",
      "1950/1950 [==============================] - 12s 6ms/step - loss: 0.3054 - acc: 0.8715 - val_loss: 0.3213 - val_acc: 0.8648\n",
      "Epoch 4/15\n",
      "1944/1950 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.8800\n",
      "Epoch 00004: val_acc improved from 0.86476 to 0.86692, saving model to best_model.h5\n",
      "1950/1950 [==============================] - 11s 6ms/step - loss: 0.2873 - acc: 0.8799 - val_loss: 0.3112 - val_acc: 0.8669\n",
      "Epoch 5/15\n",
      "1948/1950 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.8875\n",
      "Epoch 00005: val_acc improved from 0.86692 to 0.87153, saving model to best_model.h5\n",
      "1950/1950 [==============================] - 11s 6ms/step - loss: 0.2738 - acc: 0.8875 - val_loss: 0.3071 - val_acc: 0.8715\n",
      "Epoch 6/15\n",
      "1949/1950 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.8932\n",
      "Epoch 00006: val_acc improved from 0.87153 to 0.87352, saving model to best_model.h5\n",
      "1950/1950 [==============================] - 11s 5ms/step - loss: 0.2619 - acc: 0.8932 - val_loss: 0.2997 - val_acc: 0.8735\n",
      "Epoch 7/15\n",
      "1946/1950 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.8983\n",
      "Epoch 00007: val_acc did not improve from 0.87352\n",
      "1950/1950 [==============================] - 10s 5ms/step - loss: 0.2526 - acc: 0.8983 - val_loss: 0.3029 - val_acc: 0.8713\n",
      "Epoch 8/15\n",
      "1943/1950 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9026\n",
      "Epoch 00008: val_acc did not improve from 0.87352\n",
      "1950/1950 [==============================] - 11s 5ms/step - loss: 0.2424 - acc: 0.9026 - val_loss: 0.3326 - val_acc: 0.8565\n",
      "Epoch 9/15\n",
      "1943/1950 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9065\n",
      "Epoch 00009: val_acc did not improve from 0.87352\n",
      "1950/1950 [==============================] - 10s 5ms/step - loss: 0.2335 - acc: 0.9065 - val_loss: 0.3131 - val_acc: 0.8684\n",
      "Epoch 10/15\n",
      "1949/1950 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9119\n",
      "Epoch 00010: val_acc did not improve from 0.87352\n",
      "1950/1950 [==============================] - 10s 5ms/step - loss: 0.2236 - acc: 0.9119 - val_loss: 0.3387 - val_acc: 0.8690\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 2s 2ms/step - loss: 0.3044 - acc: 0.8702\n",
      "\n",
      " 테스트 정확도: 0.8702\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
