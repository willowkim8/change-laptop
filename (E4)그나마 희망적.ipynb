{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화리뷰 감성분석 도전하기\n",
    "\\$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt  \n",
    "\\$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt  \n",
    "\\$ mv ratings_*.txt ~/aiffel/sentiment_classification  \n",
    "\n",
    "$ python3\n",
    "```\n",
    ">>> from konlpy.tag import Komoran\n",
    ">>> komoran = Komoran() \n",
    "```\n",
    "에서 계속 오류가 났지만,  \n",
    "\n",
    "https://zereight.tistory.com/650  \n",
    "덕분에 Komoran을 돌릴 수 있다.\n",
    "\n",
    "https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh\n",
    "\n",
    "https://wikidocs.net/44249\n",
    "도움받음\n",
    "\n",
    "왜 단어수를 늘렸는데 정확도가 떨어질까? -> 이해함  \n",
    "그렇다면 문장의 모든 단어가 빈도수가 낮아서 <UNK>으로 변경된 data는 제거하는 게 더 좋겠습니다. 어떻게 제거하면 좋을까?  \n",
    "    \n",
    "    https://wikidocs.net/50739 WORD2VEC도움 URL  \n",
    "    https://wikidocs.net/21668 진표퍼실님이 알려준 URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 데이터 준비와 확인\n",
    "\n",
    "2) 데이터로더 구성\n",
    "\n",
    "3) 모델구성을 위한 데이터 분석 및 가공\n",
    "\n",
    "    데이터셋 내 문장 길이 분포\n",
    "    적절한 최대 문장 길이 지정\n",
    "    keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가\n",
    "\n",
    "4) 모델구성 및 validation set 구성\n",
    "\n",
    "   모델은 3가지 이상 다양하게 구성하여 실험해 보세요.\n",
    "   \n",
    "5) 모델 훈련 개시\n",
    "\n",
    "6) Loss, Accuracy 그래프 시각화\n",
    "\n",
    "7) 학습된 Embedding 레이어 분석 - 유사도 단어\n",
    "    \n",
    "8) 한국어 Word2Vec 임베딩 활용하여 성능개선\n",
    "\n",
    "   한국어 Word2Vec은 다음 경로에서 구할 수 있습니다.\n",
    "   https://github.com/Kyubyong/wordvectors\n",
    "\n",
    "평가문항\t\n",
    " 상세기준\n",
    "\n",
    "1. 다양한 방법으로 Text Classification 태스크를 성공적으로 구현하였다.\n",
    "\t3가지 이상의 모델이 성공적으로 시도됨\n",
    "\n",
    "2. gensim을 활용하여 자체학습된 혹은 사전학습된 임베딩 레이어를 분석하였다.\n",
    "\tgensim의 유사단어 찾기를 활용하여 자체학습한 임베딩과 사전학습 임베딩을 적절히 분석함\n",
    "\n",
    "3. 한국어 Word2Vec을 활용하여 가시적인 성능향상을 달성했다.\n",
    "\t네이버 영화리뷰 데이터 감성분석 정확도를 85% 이상 달성함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4dac926ffda0>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4dac926ffda0>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    break     이거 돌리면 이해실수도\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# index to text 사전\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "type(index_to_word)\n",
    "for index,word in index_to_word.items():\n",
    "    print(index,word)\n",
    "    if index == 20:\n",
    "        break     이거 돌리면 이해실수도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (E4)Sentimental_Analysis_of_Movie_Dataset_from_NAVER_InYu\n",
    "### 1) 데이터 준비와 확인 & 2) 데이터로더 구성&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 data 개수 : 150000\n",
      "테스트용 data 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import 와 read data\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "# data 개수\n",
    "print('훈련용 data 개수 :', len(train_data))\n",
    "print('테스트용 data 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인해보기\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부정 감상평 label은 0  \n",
    "긍정 감상평 lable은 1\n",
    "\n",
    "### 전처리 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비중복 data 개수\n",
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label은 0, 1뿐이라서 당연히 2개지만 150000개에서 약 4000개 정도의 중복 리뷰가 있다는 것이다.  \n",
    "중복된 리뷰를 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 리뷰 삭제\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data에서 해당 리뷰의 긍, 부정 유무가 기재되어있는 레이블(label) 값의 분포를 가시적으로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  count\n",
      "0      0  73342\n",
      "1      1  72841\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind = 'bar')\n",
    "print(train_data.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부정 감상평이 살짝 많지만 근사한 data 개수를 가진 것을 확일할 수 있었다.  \n",
    "감상평 중에 Null 값을 가진 샘플이 있는지  확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "id          0\n",
      "document    1\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>2172111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id document  label\n",
       "25857  2172111      NaN      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.isnull().values.any()) # True는 Null 값을 가진 샘플 존재한다는 뜻\n",
    "print(train_data.isnull().sum())\n",
    "train_data.loc[train_data.document.isnull()] # 목록에서 위치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "146182\n"
     ]
    }
   ],
   "source": [
    "# Null 샘플  제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # False가 나와야 Null 샘플이 없다.\n",
    "print(len(train_data)) # Null 값을 제외한 학습 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 학습용 샘플의 개수 : 145791\n"
     ]
    }
   ],
   "source": [
    "# 다시 Null값 확인하고 Null 제거\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "print('전처리 후 학습용 샘플의 개수 :',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 48995\n"
     ]
    }
   ],
   "source": [
    "# 나머지 test data도 동일하게 진행\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okt는 열심히 다운받았던 KoNLPy에서 제공하는 형태소 분석기이다. 한국어 토큰화는 띄어쓰기 기준이 아닌 형태소 분석기를 사용한다.  \n",
    "토큰화 하면서 불용어를 제거하여 x_train에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "x_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(temp_x)\n",
    "    \n",
    "x_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(temp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train에서 빈도가 높은 순으로 정수를 부여받는다. 그래서 이중에서 가장 빈도 수가 많은 9996개의 단어로 리스트를만들어서 정수로 인코딩해준다.  \n",
    "앞에는 \\<BOS>, \\<PAD>, \\<UNK>, \\<UNUSED>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 모델구성을 위한 데이터 분석 및 가공\n",
    "문장 길이를 평준화해서 벡터의 길이는 조정한다.  \n",
    "Embedding 레이어의 인풋이 되는 문장 벡터는 그 길이가 일정해야 하기 때문이다.  \n",
    "긴 문장은 자르고 짧은 단어는 \\<PAD>를 패딩해서 길이를 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 \n",
    "words = np.concatenate(x_train).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(10000-4) # 변경가능\n",
    "vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data에 있는 단어 중 9996개에 들지 못한 단어는 <UNK>으로 변경하고 words에 포함된 단어라면 인코딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145791\n",
      "48995\n",
      "145791\n",
      "48995\n",
      "[0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def wordlist_to_indexlist(wordlist):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "# y는 별도로 저장\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "# return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "# x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 길이를 평준화해서 벡터의 길이는 조정한다.  \n",
    "Embedding 레이어의 인풋이 되는 문장 벡터는 그 길이가 일정해야 하기 때문이다.  \n",
    "긴 문장은 자르고 짧은 단어는 '\\<PAD>'를 패딩해서 길이를 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  13.75715400490795\n",
      "문장길이 최대 :  83\n",
      "문장길이 표준편차 :  11.462732932420181\n",
      "pad_sequences maxlen :  42\n",
      "전체 문장의 95.41702175721048%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 전체 문장 95% 정도를 포함할 수 있게\n",
    "max_tokens = np.mean(num_tokens) + round(2.5 * np.std(num_tokens)) \n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 입력데이터가 순차적으로 처리되기 때문에 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 됩니다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적입니다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 maxlen 값에 맞춰서 패딩\n",
    "# post는 data 뒤에 패딩, pre는 data 앞에 패딩\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                      padding='pre',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 모델구성 및 validation set 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# RNN\n",
    "model_RNN = keras.Sequential()\n",
    "model_RNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_RNN.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model_RNN.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_RNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "# 1-D CNN\n",
    "model_CNN = keras.Sequential() # 동일\n",
    "model_CNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) \n",
    "model_CNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_CNN.add(keras.layers.MaxPooling1D(5))\n",
    "model_CNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_CNN.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_CNN.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model_CNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. \n",
    "\n",
    "\n",
    "# GlobalMP\n",
    "\n",
    "model_GlobalMP = keras.Sequential()\n",
    "model_GlobalMP.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_GlobalMP.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_GlobalMP.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_GlobalMP.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_RNN.summary()\n",
    "model_CNN.summary()\n",
    "model_GlobalMP.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 모델 훈련 개시 - 1. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1931/1944 [============================>.] - ETA: 0s - loss: 0.1085 - accuracy: 0.9567WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 7s 4ms/step - loss: 0.1086 - accuracy: 0.9566 - val_loss: 0.7820 - val_accuracy: 0.8269\n",
      "Epoch 2/15\n",
      "1934/1944 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9602WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 7s 3ms/step - loss: 0.0999 - accuracy: 0.9602 - val_loss: 0.8186 - val_accuracy: 0.8242\n",
      "Epoch 3/15\n",
      "1939/1944 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9636WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 7s 3ms/step - loss: 0.0933 - accuracy: 0.9636 - val_loss: 0.8664 - val_accuracy: 0.8242\n",
      "Epoch 4/15\n",
      "1937/1944 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9645WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 7s 3ms/step - loss: 0.0902 - accuracy: 0.9645 - val_loss: 0.8698 - val_accuracy: 0.8242\n",
      "Epoch 5/15\n",
      "1933/1944 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9664WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 7s 3ms/step - loss: 0.0860 - accuracy: 0.9664 - val_loss: 0.9696 - val_accuracy: 0.8225\n",
      "Epoch 00005: early stopping\n",
      "1532/1532 - 2s - loss: 0.9356 - accuracy: 0.8251\n",
      "[0.935640811920166, 0.825145423412323]\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_RNN.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history_model_RNN = model_RNN.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)\n",
    "# test 는 \"evaluate\"\n",
    "results_RNN = model_RNN.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 모델 훈련 개시 - 2. 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1932/1944 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9439WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 5s 3ms/step - loss: 0.1439 - accuracy: 0.9440 - val_loss: 0.5694 - val_accuracy: 0.8113\n",
      "Epoch 2/10\n",
      "1930/1944 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9527WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 5s 3ms/step - loss: 0.1224 - accuracy: 0.9526 - val_loss: 0.6254 - val_accuracy: 0.8082\n",
      "Epoch 3/10\n",
      "1929/1944 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9572WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 5s 3ms/step - loss: 0.1102 - accuracy: 0.9572 - val_loss: 0.6978 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "1942/1944 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9606WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 5s 3ms/step - loss: 0.1017 - accuracy: 0.9606 - val_loss: 0.7386 - val_accuracy: 0.8048\n",
      "Epoch 5/10\n",
      "1933/1944 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9639WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 5s 3ms/step - loss: 0.0936 - accuracy: 0.9638 - val_loss: 0.8084 - val_accuracy: 0.8013\n",
      "Epoch 00005: early stopping\n",
      "1532/1532 - 1s - loss: 0.8079 - accuracy: 0.8000\n",
      "[0.8079174757003784, 0.800000011920929]\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_CNN.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history_model_CNN = model_CNN.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)\n",
    "# test 는 \"evaluate\"\n",
    "results_CNN = model_CNN.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 모델 훈련 개시 - 3. GlobalMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1942/1944 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9233WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 4s 2ms/step - loss: 0.1980 - accuracy: 0.9234 - val_loss: 0.4622 - val_accuracy: 0.8276\n",
      "Epoch 2/10\n",
      "1935/1944 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 4s 2ms/step - loss: 0.1914 - accuracy: 0.9263 - val_loss: 0.4756 - val_accuracy: 0.8274\n",
      "Epoch 3/10\n",
      "1928/1944 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.9304WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 4s 2ms/step - loss: 0.1837 - accuracy: 0.9303 - val_loss: 0.4921 - val_accuracy: 0.8259\n",
      "Epoch 4/10\n",
      "1924/1944 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.9332WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 4s 2ms/step - loss: 0.1776 - accuracy: 0.9331 - val_loss: 0.5003 - val_accuracy: 0.8254\n",
      "Epoch 5/10\n",
      "1926/1944 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9359WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1944/1944 [==============================] - 4s 2ms/step - loss: 0.1719 - accuracy: 0.9356 - val_loss: 0.5202 - val_accuracy: 0.8235\n",
      "Epoch 00005: early stopping\n",
      "1532/1532 - 1s - loss: 0.5216 - accuracy: 0.8240\n",
      "[0.5216176509857178, 0.823961615562439]\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_GlobalMP.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history_model_GlobalMP = model_GlobalMP.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# test 는 \"evaluate\"\n",
    "results_GlobalMP = model_GlobalMP.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results_GlobalMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Loss, Accuracy 그래프 시각화\n",
    "3개의 모델에 대한 history 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "#history_model_RNN\n",
    "#history_model_CNN\n",
    "#history_model_GlobalMP\n",
    "history_dict_RNN = history_model_RNN.history\n",
    "history_dict_CNN = history_model_CNN.history\n",
    "history_dict_GlobalMP = history_model_GlobalMP.history\n",
    "print(history_dict_GlobalMP.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# RNN\n",
    "acc = history_dict_RNN['accuracy']\n",
    "val_acc = history_dict_RNN['val_accuracy']\n",
    "loss = history_dict_RNN['loss']\n",
    "val_loss = history_dict_RNN['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#fig, ax = plt.subplots(1, 2, sharex=True) \n",
    "# fig란 figure로써 - 전체 subplot을 말한다. \n",
    "# ex) 서브플로안에 몇개의 그래프가 있던지 상관없이  그걸 담는 하나.전체 사이즈를 말한다.\n",
    "# ax는 axe로써 - 전체 중 낱낱개를 말한다 \n",
    "# ex) 서브플롯 안에 2개(a1,a2)의 그래프가 있다면 a1, a2 를 일컬음\n",
    "\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('RNN Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('RNN Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CNN\n",
    "acc = history_dict_CNN['accuracy']\n",
    "val_acc = history_dict_CNN['val_accuracy']\n",
    "loss = history_dict_CNN['loss']\n",
    "val_loss = history_dict_CNN['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#fig, ax = plt.subplots(1, 2, sharex=True) \n",
    "# fig란 figure로써 - 전체 subplot을 말한다. \n",
    "# ex) 서브플로안에 몇개의 그래프가 있던지 상관없이  그걸 담는 하나.전체 사이즈를 말한다.\n",
    "# ax는 axe로써 - 전체 중 낱낱개를 말한다 \n",
    "# ex) 서브플롯 안에 2개(a1,a2)의 그래프가 있다면 a1, a2 를 일컬음\n",
    "\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('CNN Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('CNN Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalMP\n",
    "acc = history_dict_GlobalMP['accuracy']\n",
    "val_acc = history_dict_GlobalMP['val_accuracy']\n",
    "loss = history_dict_GlobalMP['loss']\n",
    "val_loss = history_dict_GlobalMP['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#fig, ax = plt.subplots(1, 2, sharex=True) \n",
    "# fig란 figure로써 - 전체 subplot을 말한다. \n",
    "# ex) 서브플로안에 몇개의 그래프가 있던지 상관없이  그걸 담는 하나.전체 사이즈를 말한다.\n",
    "# ax는 axe로써 - 전체 중 낱낱개를 말한다 \n",
    "# ex) 서브플롯 안에 2개(a1,a2)의 그래프가 있다면 a1, a2 를 일컬음\n",
    "\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('GlobalMP Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('GlobalMP Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) 학습된 Embedding 레이어 분석\n",
    "- 유사도 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "#history_model_RNN\n",
    "#history_model_CNN\n",
    "#history_model_GlobalMP\n",
    "embedding_layer = model_RNN.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN으로 학습된 임베딩 파라미터를 word2vec_RNN.txt에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다.\n",
    "word2vec_file_path_RNN = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec_RNN.txt'\n",
    "f = open(word2vec_file_path_RNN, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors_RNN = model_RNN.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors_RNN[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNN으로 학습된 임베딩 파라미터를 word2vec_CNN.txt에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다.\n",
    "word2vec_file_path_CNN = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec_CNN.txt'\n",
    "f = open(word2vec_file_path_CNN, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors_CNN = model_CNN.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors_CNN[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GlobalMP로 학습된 임베딩 파라미터를 word2vec_GlobalMP.txt에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다.\n",
    "word2vec_file_path_GlobalMP = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec_GlobalMP.txt'\n",
    "f = open(word2vec_file_path_GlobalMP, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors_GlobalMP = model_GlobalMP.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors_GlobalMP[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.031505  , -0.11813138,  0.16260114, -0.08221718, -0.05020014,\n",
       "        0.0510551 , -0.05689929, -0.0514534 ,  0.03904266, -0.07091825,\n",
       "        0.07267331, -0.11085432,  0.05134446,  0.32439354, -0.11647657,\n",
       "       -0.02948879], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors01 = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path_RNN, binary=False)\n",
    "word_vectors02 = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path_CNN, binary=False)\n",
    "word_vectors03 = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path_GlobalMP, binary=False)\n",
    "vector_RNN = word_vectors01['한국']\n",
    "#vector_CNN = word_vectors['한국']\n",
    "#vector_GlobalMP = word_vectors['한국']\n",
    "vector_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('새벽', 0.8940843939781189),\n",
       " ('어지간', 0.888460099697113),\n",
       " ('나부랭이', 0.8473931550979614),\n",
       " ('이룰', 0.8455268144607544),\n",
       " ('눈길', 0.8377326130867004),\n",
       " ('지려', 0.8345244526863098),\n",
       " ('제프', 0.8310742378234863),\n",
       " ('뵈', 0.8264450430870056),\n",
       " ('빨라서', 0.8205097913742065),\n",
       " ('러셀', 0.8148868083953857)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RNN학습된 임베딩\n",
    "word_vectors01.similar_by_word(\"여름\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('피자', 0.7554928064346313),\n",
       " ('작년', 0.7404083013534546),\n",
       " ('옳', 0.6893380880355835),\n",
       " ('리부트', 0.6883527040481567),\n",
       " ('미', 0.6777534484863281),\n",
       " ('귀요미', 0.6582044363021851),\n",
       " ('그려', 0.6501643657684326),\n",
       " ('이뻤', 0.6463261246681213),\n",
       " ('욜라', 0.6393013000488281),\n",
       " ('앤', 0.6355252265930176)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN학습된 임베딩\n",
    "word_vectors02.similar_by_word(\"여름\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('을', 0.9436905980110168),\n",
       " ('가득', 0.9432538151741028),\n",
       " ('서스펜스', 0.9368389844894409),\n",
       " ('블', 0.9347701072692871),\n",
       " ('또는', 0.9319551587104797),\n",
       " ('공주', 0.9311188459396362),\n",
       " ('사극', 0.9276167154312134),\n",
       " ('됐', 0.9238539338111877),\n",
       " ('거듭', 0.9203490018844604),\n",
       " ('음', 0.9190821051597595)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GlobalMP학습된 임베딩\n",
    "word_vectors03.similar_by_word(\"여름\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) 한국어 Word2Vec 임베딩 활용하여 성능개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_file_path_ko = os.getenv('HOME')+'/aiffel/ko.bin'\n",
    "word_vectors_ko = gensim.models.Word2Vec.load(word2vec_file_path_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('겨울', 0.8478226661682129),\n",
       " ('가을', 0.8154698610305786),\n",
       " ('봄', 0.6993100643157959),\n",
       " ('여름철', 0.6975582838058472),\n",
       " ('봄철', 0.663573145866394),\n",
       " ('겨울철', 0.6597992777824402),\n",
       " ('초여름', 0.6408064961433411),\n",
       " ('무덥', 0.635843813419342),\n",
       " ('방학', 0.5905724167823792),\n",
       " ('연중', 0.5791523456573486)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_ko.similar_by_word(\"여름\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련된 임베딩 모델에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"\n",
      "/home/aiffel/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (200) into shape (16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-dda3707f93f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_vectors_ko\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors_ko\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (200) into shape (16)"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word_vectors_ko:\n",
    "        embedding_matrix[i] = word_vectors_ko[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# RNN\n",
    "model_RNN = keras.Sequential()\n",
    "model_RNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_RNN.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model_RNN.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_RNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "# 1-D CNN\n",
    "model_CNN = keras.Sequential() # 동일\n",
    "model_CNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) \n",
    "model_CNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_CNN.add(keras.layers.MaxPooling1D(5))\n",
    "model_CNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_CNN.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_CNN.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model_CNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. \n",
    "\n",
    "\n",
    "# GlobalMP\n",
    "\n",
    "model_GlobalMP = keras.Sequential()\n",
    "model_GlobalMP.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_GlobalMP.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_GlobalMP.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_RNN.summary()\n",
    "model_CNN.summary()\n",
    "model_GlobalMP.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model-2. 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential() # 동일\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) # 동일\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. # 동일\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model-3. GlobalMaxPooling1D() 레이어 하나만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()\n",
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 학습된 Embedding 레이어 분석\n",
    "### Word2Vec 만들기\n",
    "### gensim\n",
    "워드벡터를 다루는데 유용함\n",
    "\n",
    "네이버 리뷰나 IMDB 리뷰는 문장들에서 긍정과 부정을 나타내는걸 알아내는 모델을 학습하는거였고 word2vec는 각 단어마다 가지고 잇는 고유벡터값 특성 표현 식?을 찾아내는데 의미가 있는거라고 하네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_file_path_ko = os.getenv('HOME')+'/aiffel/ko.bin'\n",
    "word_vectors_ko = gensim.models.Word2Vec.load(word2vec_file_path_ko)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 훈련시키기\n",
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('/home/aiffel/aiffel/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.wv.most_similar(\"사랑\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30185    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 ?????? 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('/home/aiffel/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('/home/aiffel/aiffel/sentiment_classification/ratings_test.txt')\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL 값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비중복 data 개수\n",
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 리뷰 삭제\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label'].value_counts().plot(kind = 'bar')\n",
    "print(train_data.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 Null값 확인하고 Null 제거\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "print('전처리 후 학습용 샘플의 개수 :',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나머지 test data도 동일하게 진행\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "\n",
    "tokenizer = Mecab()\n",
    "x_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(temp_x)\n",
    "    \n",
    "x_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 \n",
    "words = np.concatenate(x_train).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(10000-4) # 변경가능\n",
    "vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlist_to_indexlist(wordlist):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "# y는 별도로 저장\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 전체 문장 95% 정도를 포함할 수 있게\n",
    "max_tokens = np.mean(num_tokens) + round(2.5 * np.std(num_tokens)) \n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 maxlen 값에 맞춰서 패딩\n",
    "# post는 data 뒤에 패딩, pre는 data 앞에 패딩\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                      padding='pre',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "# vector = word2vec['computer']\n",
    "# print(len(vector))\n",
    "# vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec에서 제공하는 워드 임베딩 벡터들끼리는 의미적 유사도가 가까운 것이 서로 가깝게 제대로 학습된 것을 확인할 수 있습니다. 이제 우리는 이전 스텝에서 학습했던 모델의 임베딩 레이어를 Word2Vec의 것으로 교체하여 다시 학습시켜 볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model-4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 8  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation loss를 그려 보면, 몇 epoch까지의 트레이닝이 적절한지 최적점을 추정해 볼 수 있습니다. validation loss의 그래프가 train loss와의 이격이 발생하게 되면 더이상의 트레이닝은 무의미해지게 마련입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 학습된 Embedding 레이어 분석\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "print(len(vector))\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec에서 제공하는 워드 임베딩 벡터들끼리는 의미적 유사도가 가까운 것이 서로 가깝게 제대로 학습된 것을 확인할 수 있습니다. 이제 우리는 이전 스텝에서 학습했던 모델의 임베딩 레이어를 Word2Vec의 것으로 교체하여 다시 학습시켜 볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "prtl_x_train = x_train[10000:]\n",
    "prtl_y_train = y_train[10000:]\n",
    "\n",
    "epochs=15  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(prtl_x_train,\n",
    "                    prtl_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "              \n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    callbacks=[es, mc],\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 데이터로더 구성\n",
    "\n",
    "실습때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.\n",
    "\n",
    "    데이터의 중복 제거\n",
    "    NaN 결측치 제거\n",
    "    한국어 토크나이저로 토큰화\n",
    "    불용어(Stopwords) 제거\n",
    "    사전word_to_index 구성\n",
    "    텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "    X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[3]) # 왜 BOS로 시작 안하지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[:])\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "\n",
    "print(x_train[3])\n",
    "print(get_decoded_sentence(x_train[3], index_to_word))\n",
    "print('라벨: ', y_train[3])  # 6번째 리뷰데이터의 라벨 부정은 0, 긍정은 1\n",
    "\n",
    "# x_train에 아직 BOS 안 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[8410])\n",
    "print(index_to_word[158]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('아이언맨 영화 좋다', word_to_index))\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "encoded_sentences = get_encoded_sentences('개노잼', word_to_index) # 이해안감\n",
    "print(encoded_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금 보니까 보정 안하는 게 맞음\n",
    "# # 보정 -> 실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "# word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# # 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "# word_to_index[\"<PAD>\"] = 0\n",
    "# word_to_index[\"<BOS>\"] = 1\n",
    "# word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "# word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# index_to_word[0] = \"<PAD>\"\n",
    "# index_to_word[1] = \"<BOS>\"\n",
    "# index_to_word[2] = \"<UNK>\"\n",
    "# index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "# index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "# print(x_train[3])\n",
    "# print(get_decoded_sentence(x_train[3], index_to_word))\n",
    "# print('라벨: ', y_train[3])  # 6번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[0]) \n",
    "print(index_to_word[1]) \n",
    "print(index_to_word[2]) \n",
    "print(index_to_word[3])  # padding 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 3\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 500\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding 숫자 출력할 때 왜 1이 안나올까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # for RNN\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                     padding='pre', # for RNN\n",
    "                                                      maxlen=maxlen)\n",
    "\n",
    "print('(x_train data 개수, 각 data의 length) =', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 3\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - RNN\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) # 동일 # model의 첫번째 레이어\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. # 동일\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 분리\n",
    "x_val = x_train[:30000]   \n",
    "y_val = y_train[:30000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[30000:]  \n",
    "partial_y_train = y_train[30000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1000,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential() # 동일\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) # 동일\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. # 동일\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1500,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post로 pad 하고 진행해보기\n",
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                     padding='post',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 3\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1500,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobalMaxPooling1D() 레이어 하나만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GlobalMaxPooling1D() 레이어 하나만\n",
    "\n",
    "# post로 pad 하고 진행해보기\n",
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                     padding='post',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1500,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제일 높게 나온 Global pre로 진행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre로 pad 하고 진행해보기\n",
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                     padding='pre',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1500,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
