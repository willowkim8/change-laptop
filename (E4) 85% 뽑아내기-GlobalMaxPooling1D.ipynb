{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146182\n",
      "49157\n"
     ]
    }
   ],
   "source": [
    "### GlobalMaxPooling1D() 레이어 하나만\n",
    "\n",
    "# post로 pad 하고 진행해보기\n",
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) \n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(5000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n",
      "pad_sequences maxlen :  47\n",
      "전체 문장의 95.2175448835102%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + round(2.5 * np.std(num_tokens))\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                     padding='post',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 8)           40000     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 40,081\n",
      "Trainable params: 40,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 8  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbose: Integer. 0, 1, or 2. \n",
    "Verbosity mode. \n",
    "\n",
    "\n",
    "0 = silent, \n",
    "1 = progress bar, \n",
    "2 = one line per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Epoch 1/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.4627 - accuracy: 0.7853 - val_loss: 0.3816 - val_accuracy: 0.8304\n",
      "Epoch 2/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.3568 - accuracy: 0.8435 - val_loss: 0.3700 - val_accuracy: 0.8390\n",
      "Epoch 3/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.3347 - accuracy: 0.8547 - val_loss: 0.3696 - val_accuracy: 0.8379\n",
      "Epoch 4/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.3214 - accuracy: 0.8620 - val_loss: 0.3706 - val_accuracy: 0.8392\n",
      "Epoch 5/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.3122 - accuracy: 0.8669 - val_loss: 0.3733 - val_accuracy: 0.8394\n",
      "Epoch 6/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.3050 - accuracy: 0.8695 - val_loss: 0.3765 - val_accuracy: 0.8379\n",
      "Epoch 7/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2990 - accuracy: 0.8723 - val_loss: 0.3784 - val_accuracy: 0.8380\n",
      "Epoch 8/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2941 - accuracy: 0.8759 - val_loss: 0.3828 - val_accuracy: 0.8369\n",
      "Epoch 9/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2900 - accuracy: 0.8770 - val_loss: 0.3842 - val_accuracy: 0.8362\n",
      "Epoch 10/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2861 - accuracy: 0.8785 - val_loss: 0.3886 - val_accuracy: 0.8369\n",
      "Epoch 11/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2827 - accuracy: 0.8816 - val_loss: 0.3913 - val_accuracy: 0.8348\n",
      "Epoch 12/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2795 - accuracy: 0.8825 - val_loss: 0.3958 - val_accuracy: 0.8333\n",
      "Epoch 13/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2767 - accuracy: 0.8842 - val_loss: 0.3979 - val_accuracy: 0.8327\n",
      "Epoch 14/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2741 - accuracy: 0.8857 - val_loss: 0.4013 - val_accuracy: 0.8325\n",
      "Epoch 15/15\n",
      "1950/1950 [==============================] - 3s 2ms/step - loss: 0.2716 - accuracy: 0.8870 - val_loss: 0.4027 - val_accuracy: 0.8337\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print('---------')\n",
    "              \n",
    "epochs=15  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=60,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 1s - loss: 0.4068 - accuracy: 0.8313\n",
      "[0.4067894518375397, 0.8312956690788269]\n"
     ]
    }
   ],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
