{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (E1)rock_scissor_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비  Preparing your data\n",
    "\n",
    "직접 가위, 바위, 보를 100장씩 찍은 사진을 준비합시다.   \n",
    "Let's take photos of each rock, scissor, paper yourself.\n",
    "\n",
    "저 또한 어느 세월에 찍고 있나 했지만, 역시 구글은 다 준비를 해놨습니다.  \n",
    "I also had worried before taking the pictures but Google prepared everything.  \n",
    "https://teachablemachine.withgoogle.com/\n",
    "에 들어가서 여러 각도, 여러 크기로 연속으로 사진을 찍기 바랍니다.  \n",
    "After following the URL, please take continuous pictures of different angles and sizes.  \n",
    "\n",
    "이미지를 저장하고 속성에 들어가서 크기를 확인하면 224X224 픽셀인 것을 알 수 있습니다.  \n",
    "If you save the images and enter the property to see its size, you can see that it is 224X224 pixels.  \n",
    "이 이미지를 28X28 픽셀로 resize해서 딥러닝이 가볍게 돌아갈 수 있도록 해줍시다.  \n",
    "For doing deep learning lightly, you have to resize the images into 28X28 pixels.  \n",
    "이때 PIL 라이브러리를 이용합니다.  \n",
    "We're going to use the PIL library.\n",
    "+ Python에서 이미지 처리를 할 때 사용하는 라이브러리 중 하나입니다. \n",
    "+ That is one of the libraries that Python uses to process images.  \n",
    "\n",
    "또, os 와 glob 모듈을 사용할 것입니다. 모듈은 함수나 변수 또는 클래스를 모아 놓은 파일입니다.  \n",
    "We're going to use os and glob modules too. The module is the pile that gathered variables or classes.  \n",
    "우리는 파이썬이 이미 모아둔 파일을 이용하기만 하면 됩니다.  \n",
    "We can just take advantage of the pile that was gathered by Python.\n",
    "\n",
    "+ os 모듈은 Operating System의 약자로서 운영체제에서 제공되는 여러 기능을 파이썬에서 수행할 수 있게 해줍니다. 예를 들어, 파이썬을 이용해 파일을 복사하거나 디렉터리를 생성하고 특정 디렉터리 내의 파일 목록을 구하고자 할 때 os 모듈을 사용하면 됩니다.\n",
    "+ The os modules are abbreviated to Operating System, allowing Python to perform many of the functions provided by the operating system. For example, if you want to use Python to copy files, create directories, and get a list of files within a specific directory, you can use the os module.  \n",
    "\n",
    "+ glob 모듈은 윈도우의 dir 명령어나 리눅스의 ls 명령어와 유사한 기능을 제공합니다. glob() 함수는 경로에 대응되는 모든 파일 및 디렉터리의 리스트를 반환합니다.\n",
    "+ The glob module provides functionality similar to the dir command in Windows or ls command in Linux. The glob() function returns a list of all files and directories corresponding to the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/rock_scissor_paper/scissor\n",
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/rock_scissor_paper/rock\n",
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/rock_scissor_paper/paper\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, glob #easy to use, thank you Python!\n",
    "\n",
    "# size 변경하기\n",
    "# Let's change size of the images\n",
    "\n",
    "# 사진이 저장된 폴더(디렉토리)에 있는 jpg 파일들을 28x28 사이즈로 바꾸어 저장합니다.\n",
    "# Read all jpg files under the directory where the scissor image is stored and save each piles in 28x28 size.\n",
    "\n",
    "# scissor\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\" # resize할 사진이 저장된 경로\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "\n",
    "images=glob.glob(image_dir_path + \"/*.jpg\")\n",
    "target_size=(28,28)\n",
    "for img in images: # 반복문으로 모든 이미지에 대해서 아래와 같은 일을 수행\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "    \n",
    "# 바위 와 보에도 똑같이 적용합니다.\n",
    "# Do same at rock and paper.\n",
    "\n",
    "#rock\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "images=glob.glob(image_dir_path + \"/*.jpg\")  \n",
    "\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "    \n",
    "# paper\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "images=glob.glob(image_dir_path + \"/*.jpg\")  \n",
    "\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 딥러닝 네트워크 설계\n",
    "다음 코드는 텐서플로우 케라스의 여러 방법 중 Sequential API를 이용한 것입니다.  \n",
    "Following codes is Sequential API of TensorFlow keras's method.  \n",
    "CS231n 수업에서 접할 수 있는 LeNet을 설계한 예입니다.  \n",
    "This is an example of the design of LeNet that can be see at class CS231n.  \n",
    "간단해 보이지만 가위, 바위, 보 분류기를 구현하는 데는 충분합니다.  \n",
    "It looks very simple but it can clssify rock, scissor and paper enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_93 (Conv2D)           (None, 26, 26, 30)        840       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_93 (MaxPooling (None, 13, 13, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 11, 11, 60)        16260     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_94 (MaxPooling (None, 5, 5, 60)          0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 60)                90060     \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 3)                 183       \n",
      "=================================================================\n",
      "Total params: 107,343\n",
      "Trainable params: 107,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(30, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(60, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(60, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 라벨링 Labeling\n",
    "사이즈를 줄인 사진에게 label을 붙여봅시다.  \n",
    "Let's label the photos that are reduced in size.  \n",
    "라벨은 간단하게 숫자를 이용하겠습니다.  \n",
    "We will simply use numbers for the label.  \n",
    "가위 : 0, 바위 : 1, 보 : 2  \n",
    "scissor : 0, rock : 1, paper : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path):\n",
    "\n",
    "    # 디렉토리에 들어있는 총 가위바위보 이미지 개수와 정확히 일치해야 합니다.\n",
    "    # The total number of Rock-Paper-Scissors images in the directory must match exactly.\n",
    "    number_of_data=4200    \n",
    "    img_size=28\n",
    "    color=3\n",
    "    \n",
    "    # 각 이미지 데이터와 라벨 데이터를 담을 행렬(matrix) 영역만 생성합니다.\n",
    "    # Create only a matrix area to hold each image data and label data.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        # 데이터 영역에 이미지 행렬을 복사\n",
    "        # Copy the image matrix to the data region.\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.학습시키기 Training\n",
    "라벨링까지 완료한 데이터로 x_train 학습시키기  \n",
    "Traing x_train with labeled data  \n",
    "epochs : 전체 데이터를 학습한 정도, epoch 값이 너무 작다면 underfitting이 너무 크다면 overfitting이 발생할 확률이 높아집니다.  \n",
    "epochs : The degree to which you learned the whole data, if the epoch value is too small, underfitting, if too large, is more likely to occur.  \n",
    "batch_size : 한 번의 batch마다 주는 데이터 샘플의 size  \n",
    "batch_size : Size of data samples given per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 4200 입니다.\n",
      "Epoch 1/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.9965 - accuracy: 0.4764\n",
      "Epoch 2/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.7090\n",
      "Epoch 3/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.4315 - accuracy: 0.8240\n",
      "Epoch 4/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.2750 - accuracy: 0.8936\n",
      "Epoch 5/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1759 - accuracy: 0.9438\n",
      "Epoch 6/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1185 - accuracy: 0.9645\n",
      "Epoch 7/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0667 - accuracy: 0.9843\n",
      "Epoch 8/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0778 - accuracy: 0.9764\n",
      "Epoch 9/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0387 - accuracy: 0.9914\n",
      "Epoch 10/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9974\n",
      "Epoch 11/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0090 - accuracy: 0.9998\n",
      "Epoch 12/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0084 - accuracy: 0.9990\n",
      "Epoch 13/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9998\n",
      "Epoch 15/15\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1125 - accuracy: 0.9671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3e1c1ba090>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path) # load data\n",
    "x_train_norm = x_train/255.0   # Normalization \n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=15, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.테스트 데이터로 성능확인  Evaluation\n",
    "학습 단계처럼 data를 load할 matrix를 생성하고 evaluate으로 테스트 데이타를 확인한다.  \n",
    "Create a matrix to load the data as in the Training phase and verify the test data with evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터(x_test)의 이미지 개수는 900 입니다.\n",
      "29/29 - 0s - loss: 2.7167 - accuracy: 0.6567\n",
      "test_loss: 2.7166683673858643 \n",
      "test_accuracy: 0.6566666960716248\n"
     ]
    }
   ],
   "source": [
    "def load_data(img_path):\n",
    "    number_of_data=900\n",
    "    img_size=28\n",
    "    color=3\n",
    "    \n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "   \n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img  \n",
    "        labels[idx]=1  \n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=2   \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트 데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "test_image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(test_image_dir_path) \n",
    "x_test_norm = x_test/255.0   \n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1.알맞게 추론한 데이터 톺아보기 See how well I inferred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.predict() 결과 :  [9.9933213e-01 2.6418080e-04 4.0360400e-04]\n",
      "model이 추론한 가장 가능성이 높은 결과 :  0\n",
      "실제 데이터의 라벨 :  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨: 0, 예측결과: 0\n"
     ]
    }
   ],
   "source": [
    "predicted_result = model.predict(x_test_norm)  # model이 추론한 확률값. \n",
    "predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "idx=0  #1번째 x_test를 살펴보자. \n",
    "print('model.predict() 결과 : ', predicted_result[idx])\n",
    "print('model이 추론한 가장 가능성이 높은 결과 : ', predicted_labels[idx])\n",
    "print('실제 데이터의 라벨 : ', y_test[idx])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_test[idx],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(\"라벨: \" + str(y_test[idx]) + \", 예측결과: \" + str(predicted_labels[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2.잘못 추론한 데이터 톺아보기 See incorrectly inferred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측확률분포: [1.9611792e-01 8.1039725e-05 8.0380100e-01]\n",
      "예측결과: 2, 라벨: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측확률분포: [4.4452162e-15 4.3511132e-04 9.9956483e-01]\n",
      "예측결과: 2, 라벨: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "wrong_predict_list=[]\n",
    "for i, _ in enumerate(predicted_labels):\n",
    "    # i번째 test_labels과 y_test이 다른 경우만 모아 봅시다. \n",
    "    if predicted_labels[i] != y_test[i]:\n",
    "        wrong_predict_list.append(i)\n",
    "\n",
    "# wrong_predict_list 에서 랜덤하게 5개만 뽑아봅시다.\n",
    "samples = random.choices(population=wrong_predict_list, k=2)\n",
    "\n",
    "for n in samples:\n",
    "    print(\"예측확률분포: \" + str(predicted_result[n]))\n",
    "    print(\"예측결과: \" + str(predicted_labels[n]) + \", 라벨: \" + str(y_test[n]))\n",
    "    plt.imshow(x_test[n], cmap=plt.cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.더 좋은 성능을 위해 hyperparameter 변경 Change hyperparameter\n",
    "#### 7-1.overfitting 되었는지 확인해보기 위해  To make sure it's overfitting,\n",
    "Conv2D layer에서 입력 이미지 특징 개수를 줄이고  \n",
    "Reduce the number of input image features in the Conv2D layer,  \n",
    "Dense layer에서 뉴런 개수를 줄이고  \n",
    "reduce the number of neurons in the Dense layer,  \n",
    "epoch를 줄여서 전체 데이터 훈련 횟수를 줄이고  \n",
    "reduce the number of total data training in epoch,  \n",
    "batch_size를 줄여 batch마다 데이터 샘플의 size을 줄였습니다.  \n",
    "and reduce the number of data sample's size per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_95 (Conv2D)           (None, 26, 26, 15)        420       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_95 (MaxPooling (None, 13, 13, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 11, 11, 30)        4080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_96 (MaxPooling (None, 5, 5, 30)          0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 30)                22530     \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 27,123\n",
      "Trainable params: 27,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "4200/4200 [==============================] - 3s 746us/step - loss: 1.4185 - accuracy: 0.4707\n",
      "Epoch 2/7\n",
      "4200/4200 [==============================] - 3s 741us/step - loss: 0.8805 - accuracy: 0.5821\n",
      "Epoch 3/7\n",
      "4200/4200 [==============================] - 3s 738us/step - loss: 0.7176 - accuracy: 0.6762\n",
      "Epoch 4/7\n",
      "4200/4200 [==============================] - 3s 737us/step - loss: 0.5843 - accuracy: 0.7481\n",
      "Epoch 5/7\n",
      "4200/4200 [==============================] - 3s 734us/step - loss: 0.5101 - accuracy: 0.7933\n",
      "Epoch 6/7\n",
      "4200/4200 [==============================] - 3s 737us/step - loss: 0.4183 - accuracy: 0.8440\n",
      "Epoch 7/7\n",
      "4200/4200 [==============================] - 3s 738us/step - loss: 0.3563 - accuracy: 0.8610\n",
      "학습데이터(x_test)의 이미지 개수는 900 입니다.\n",
      "29/29 - 0s - loss: 2.9913 - accuracy: 0.5011\n",
      "test_loss: 2.9913086891174316 \n",
      "test_accuracy: 0.5011110901832581\n"
     ]
    }
   ],
   "source": [
    "# 바꿔 볼 수 있는 하이퍼파라미터들\n",
    "# changable hyperparameter\n",
    "# original n_channel_1=30\n",
    "# original n_channel_2=60\n",
    "# original n_dense=60\n",
    "# original n_train_epoch=15\n",
    "# original batch_size=20\n",
    "\n",
    "n_channel_1=15\n",
    "n_channel_2=30\n",
    "n_dense=30\n",
    "n_train_epoch=7\n",
    "n_batch_size=1\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=n_train_epoch, batch_size=n_batch_size)\n",
    "\n",
    "def load_data_for_test(img_path):\n",
    "   \n",
    "    number_of_data=900   \n",
    "    img_size=28\n",
    "    color=3\n",
    "\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img   \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img   \n",
    "        labels[idx]=1  \n",
    "        idx=idx+1       \n",
    "\n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img   \n",
    "        labels[idx]=2   \n",
    "        idx=idx+1\n",
    "\n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "test_image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data_for_test(test_image_dir_path)\n",
    "x_test_norm = x_test/255.0 \n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-1.underfitting 되었는지 확인해보기 위해  To make sure it's underfitting,\n",
    "위와 반대로 모두 늘여서 test했습니다.  \n",
    "I tested it by stretching it out as opposed to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_97 (Conv2D)           (None, 26, 26, 60)        1680      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_97 (MaxPooling (None, 13, 13, 60)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 11, 11, 120)       64920     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_98 (MaxPooling (None, 5, 5, 120)         0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 120)               360120    \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 3)                 363       \n",
      "=================================================================\n",
      "Total params: 427,083\n",
      "Trainable params: 427,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 4.1137 - accuracy: 0.4729\n",
      "Epoch 2/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.5945 - accuracy: 0.7367\n",
      "Epoch 3/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.3818 - accuracy: 0.8367\n",
      "Epoch 4/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.2478 - accuracy: 0.9029\n",
      "Epoch 5/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.2114 - accuracy: 0.9202\n",
      "Epoch 6/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.1178 - accuracy: 0.9595\n",
      "Epoch 7/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0692 - accuracy: 0.9748\n",
      "Epoch 8/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0800 - accuracy: 0.9733\n",
      "Epoch 9/20\n",
      "105/105 [==============================] - 1s 12ms/step - loss: 0.0671 - accuracy: 0.9769\n",
      "Epoch 10/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.1031 - accuracy: 0.9683\n",
      "Epoch 11/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0530 - accuracy: 0.9848\n",
      "Epoch 12/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0158 - accuracy: 0.9955\n",
      "Epoch 13/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0045 - accuracy: 0.9995\n",
      "Epoch 14/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0153 - accuracy: 0.9950\n",
      "Epoch 15/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0165 - accuracy: 0.9962\n",
      "Epoch 16/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.1613 - accuracy: 0.9531\n",
      "Epoch 17/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0805 - accuracy: 0.9743\n",
      "Epoch 18/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0220 - accuracy: 0.9940\n",
      "Epoch 19/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 20/20\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.0040 - accuracy: 0.9990\n",
      "학습데이터(x_test)의 이미지 개수는 900 입니다.\n",
      "29/29 - 0s - loss: 4.6658 - accuracy: 0.5122\n",
      "test_loss: 4.665750980377197 \n",
      "test_accuracy: 0.5122222304344177\n"
     ]
    }
   ],
   "source": [
    "#바꿔 볼 수 있는 하이퍼파라미터들\n",
    "# changable hyperparameter\n",
    "# original n_channel_1=30\n",
    "# original n_channel_2=60\n",
    "# original n_dense=60\n",
    "# original n_train_epoch=15\n",
    "# original batch_size=20\n",
    "\n",
    "n_channel_1=60\n",
    "n_channel_2=120\n",
    "n_dense=120\n",
    "n_train_epoch=20\n",
    "n_batch_size=40\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=n_train_epoch, batch_size=n_batch_size)\n",
    "\n",
    "def load_data_for_test(img_path):\n",
    "   \n",
    "    number_of_data=900   \n",
    "    img_size=28\n",
    "    color=3\n",
    "  \n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0 \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img   \n",
    "        labels[idx]=1   \n",
    "        idx=idx+1       \n",
    "\n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=2  \n",
    "        idx=idx+1\n",
    "\n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "test_image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data_for_test(test_image_dir_path)\n",
    "x_test_norm = x_test/255.0   \n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.고찰 Consideration\n",
    "최대한 많은 data를 모은 덕분에 첫 test에 비교적 안정적인 결과 나왔다고 생각한다.  \n",
    "I think the results were relatively stable in the first test because I collected as much data as possible.  \n",
    "이후 overfitting과 underfitting을 확인해 보았지만 첫 테스트의 정확도가 더 높았다.  \n",
    "Then I checked overfitting and underfitting, but the first test was more accurate.  \n",
    "왜 첫 test의 정확도가 제일 높았는지 생각해보자.  \n",
    "Let's think about why the accuracy of the first test was the highest.\n",
    "- 작은 픽셀 이미지에 맞는 Conv2D와 Dense를 MNIST를 참고해서 적절하게 구했다. \n",
    "- Conv2D and Dense for small pixel images were appropriately obtained by referring to MNIST.\n",
    "- train data와 test data의 비율을 Exploration 2를 참고해 8:2의 비율로 진행했다.  \n",
    "- The ratio of train data and test data was carried out at a ratio of 8:2 with reference to Exploration 2.  \n",
    "이렇게 2가지를 꼽을 수 있겠다.  \n",
    "I can pick these two.  \n",
    "  \n",
    "다음 학습에는 본래 코드를 더 이해하면서 응용할 수 있도록 노력해야겠다.  \n",
    "In the next learning, I should try to apply the original code with a better understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
