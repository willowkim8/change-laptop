{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (E4)Sentimental_Analysis_of_Movie_Dataset_from_NAVER_InYu\n",
    "### 순서\n",
    "\n",
    "### 1) 데이터 준비와 확인\n",
    "\n",
    "2) 데이터로더 구성\n",
    "\n",
    "3) 모델구성을 위한 데이터 분석 및 가공\n",
    "\n",
    "    데이터셋 내 문장 길이 분포\n",
    "    적절한 최대 문장 길이 지정\n",
    "    keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가\n",
    "\n",
    "4) 모델구성 및 validation set 구성\n",
    "\n",
    "   모델은 3가지 이상 다양하게 구성하여 실험해 보세요.\n",
    "   \n",
    "5) 모델 훈련 개시\n",
    "\n",
    "6) Loss, Accuracy 그래프 시각화\n",
    "\n",
    "7) 학습된 Embedding 레이어 분석 - 유사도 단어\n",
    "    \n",
    "8) 한국어 Word2Vec 임베딩 활용하여 성능개선\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 data 개수 : 150000\n",
      "테스트용 data 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import 와 read data\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "# data 개수\n",
    "print('훈련용 data 개수 :', len(train_data))\n",
    "print('테스트용 data 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {2} 데이터 확인해보기\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부정 감상평 label은 0  \n",
    "긍정 감상평 lable은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비중복 data 개수\n",
    "```train_data['document'].nunique(), train_data['label'].nunique()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label은 0, 1뿐이라서 당연히 2개지만 150000개에서 약 4000개 정도의 중복 리뷰가 있다는 것이다.  \n",
    "중복된 리뷰를 삭제한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중복 리뷰 삭제\n",
    "```train_data.drop_duplicates(subset=['document'], inplace=True)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data에서 해당 리뷰의 긍, 부정 유무가 기재되어있는 레이블(label) 값의 분포를 가시적으로 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_data['label'].value_counts().plot(kind = 'bar')\n",
    "print(train_data.groupby('label').size().reset_index(name = 'count'))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부정 감상평이 살짝 많지만 근사한 data 개수를 가진 것을 확일할 수 있었다.  \n",
    "감상평 중에 Null 값을 가진 샘플이 있는지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```print(train_data.isnull().values.any()) # True는 Null 값을 가진 샘플 존재한다는 뜻\n",
    "print(train_data.isnull().sum())\n",
    "train_data.loc[train_data.document.isnull()] # 목록에서 위치 확인```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null 샘플  제거\n",
    "```train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # False가 나와야 Null 샘플이 없다.\n",
    "print(len(train_data)) # Null 값을 제외한 학습 data```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "```\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다시 Null값 확인하고 Null 제거\n",
    "```train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "print('전처리 후 학습용 샘플의 개수 :',len(train_data))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나머지 test data도 동일하게 진행\n",
    "```test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 제거\n",
    "```stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "okt.morphs('이 영화 진짜 인생영화임', stem = True)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okt는 열심히 다운받았던 KoNLPy에서 제공하는 형태소 분석기이다. 한국어 토큰화는 띄어쓰기 기준이 아닌 형태소 분석기를 사용한다.  \n",
    "토큰화 하면서 불용어를 제거하여 x_train에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tokenizer = Mecab()\n",
    "x_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(temp_x)\n",
    "    \n",
    "x_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(temp_x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train에서 빈도가 높은 순으로 정수를 부여받는다. 그래서 이중에서 가장 빈도 수가 많은 9996개의 단어로 리스트를만들어서 정수로 인코딩해준다.  \n",
    "앞에는 \\<BOS>, \\<PAD>, \\<UNK>, \\<UNUSED>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩 \n",
    "```words = np.concatenate(x_train).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(10000-4) # 변경가능\n",
    "vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 데이터로더 구성\n",
    "전처리를 모두 끝낸 데이터를 x_train, y_train, x_test, y_test에 각각 load한다.\n",
    "\n",
    "data에 있는 단어 중 9996개에 들지 못한 단어는 <UNK>으로 변경하고 words에 포함된 단어라면 인코딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) \n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wordlist_to_indexlist(wordlist):\n",
    "#     return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "# x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "# x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "# # y는 별도로 저장\n",
    "# y_train = np.array(train_data['label'])\n",
    "# y_test = np.array(test_data['label'])\n",
    "\n",
    "# print(len(x_train))\n",
    "# print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 모델구성을 위한 데이터 분석 및 가공\n",
    "문장 길이를 평준화해서 벡터의 길이는 조정한다.  \n",
    "Embedding 레이어의 인풋이 되는 문장 벡터는 그 길이가 일정해야 하기 때문이다.  \n",
    "긴 문장은 자르고 짧은 단어는 \\<PAD>를 패딩해서 길이를 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n",
      "pad_sequences maxlen :  47\n",
      "전체 문장의 95.2175448835102%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "# 데이터셋 내 문장 길이 분포 확인\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 위의 연산값을 이용해서 전체 문장 95% 정도를 포함할 수 있게 적절한 최대 문장 길이 지정\n",
    "max_tokens = np.mean(num_tokens) + round(2.5 * np.std(num_tokens)) \n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 입력데이터가 순차적으로 처리되기 때문에 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 됩니다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적입니다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 maxlen 값에 맞춰서 패딩\n",
    "# post는 data 뒤에 패딩, pre는 data 앞에 패딩\n",
    "# keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                      padding='pre',\n",
    "                                                      maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 모델구성 및 validation set 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,129\n",
      "Trainable params: 162,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "#Conv1D\n",
    "model_CNN = keras.Sequential()\n",
    "model_CNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_CNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_CNN.add(keras.layers.MaxPooling1D(5))\n",
    "model_CNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_CNN.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_CNN.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_CNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "#GlobalMaxPooling1D() 레이어 하나만 사용하는 방법\n",
    "#전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출하여 그것으로 문장의 긍정/부정을 평가하는 방식\n",
    "model_GlobMP = keras.Sequential()\n",
    "model_GlobMP.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_GlobMP.add(keras.layers.GlobalMaxPooling1D())\n",
    "#model_GlobMP.add(keras.layers.Dropout(0.3))\n",
    "model_GlobMP.add(keras.layers.Dense(8, activation='relu'))\n",
    "#model_GlobMP.add(keras.layers.Dropout(0.3))\n",
    "model_GlobMP.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "# RNN\n",
    "model_RNN = keras.Sequential()\n",
    "model_RNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) # 동일 # model의 첫번째 레이어\n",
    "model_RNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_RNN.add(keras.layers.MaxPooling1D(5))\n",
    "model_RNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_RNN.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_RNN.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model_RNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. # 동일\n",
    "\n",
    "\n",
    "#LSTM\n",
    "model_LSTM = keras.Sequential()\n",
    "model_LSTM.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_LSTM.add(keras.layers.SpatialDropout1D(0.4))\n",
    "model_LSTM.add(keras.layers.LSTM(word_vector_dim, dropout=0.2, recurrent_dropout=0.2))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다.\n",
    "model_LSTM.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 100))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_CNN.summary()\n",
    "model_GlobMP.summary()\n",
    "model_RNN.summary()\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 47)\n",
      "(146182,)\n",
      "(136182, 47)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 모델 훈련 개시\n",
    "#### 1. 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 9s 33ms/step - loss: 0.5095 - accuracy: 0.7398 - val_loss: 0.3629 - val_accuracy: 0.8363\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.3422 - accuracy: 0.8503 - val_loss: 0.3521 - val_accuracy: 0.8443\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.3079 - accuracy: 0.8691 - val_loss: 0.3476 - val_accuracy: 0.8482\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.2799 - accuracy: 0.8833 - val_loss: 0.3525 - val_accuracy: 0.8469\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.8990 - val_loss: 0.3619 - val_accuracy: 0.8442\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.2202 - accuracy: 0.9132 - val_loss: 0.3838 - val_accuracy: 0.8421\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1901 - accuracy: 0.9280 - val_loss: 0.4111 - val_accuracy: 0.8392\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1642 - accuracy: 0.9397 - val_loss: 0.4541 - val_accuracy: 0.8368\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1413 - accuracy: 0.9501 - val_loss: 0.4937 - val_accuracy: 0.8340\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1224 - accuracy: 0.9573 - val_loss: 0.5515 - val_accuracy: 0.8273\n",
      "1537/1537 - 4s - loss: 0.5680 - accuracy: 0.8269\n",
      "[0.5679938197135925, 0.8269422650337219]\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_CNN.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_model_CNN= model_CNN.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "                           \n",
    "# 테스트셋을 통한 모델 평가\n",
    "results = model_CNN.evaluate(x_test,  y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. GlobMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0387 - accuracy: 0.9860 - val_loss: 1.3454 - val_accuracy: 0.8050\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0327 - accuracy: 0.9886 - val_loss: 1.3977 - val_accuracy: 0.8045\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.0310 - accuracy: 0.9893 - val_loss: 1.4531 - val_accuracy: 0.8034\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.0300 - accuracy: 0.9897 - val_loss: 1.5381 - val_accuracy: 0.8025\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.0291 - accuracy: 0.9900 - val_loss: 1.5738 - val_accuracy: 0.8013\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0285 - accuracy: 0.9901 - val_loss: 1.6338 - val_accuracy: 0.8004\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0279 - accuracy: 0.9902 - val_loss: 1.6973 - val_accuracy: 0.8007\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0275 - accuracy: 0.9902 - val_loss: 1.7572 - val_accuracy: 0.7990\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0270 - accuracy: 0.9904 - val_loss: 1.8121 - val_accuracy: 0.7993\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0267 - accuracy: 0.9905 - val_loss: 1.8816 - val_accuracy: 0.7989\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"(<class 'list'> containing values of types set())\", '(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-36dd14749acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 테스트셋을 통한 모델 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresults_GlobMP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_GlobMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_GlobMP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1055\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/yes/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 963\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    964\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"(<class 'list'> containing values of types set())\", '(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_GlobMP.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_model_GlobMP= model_CNN.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "                           \n",
    "# 테스트셋을 통한 모델 평가\n",
    "results_GlobMP = model_GlobMP.evaluate(x_test,  y_test, verbose=2)\n",
    "print(results_GlobMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0531 - accuracy: 0.9825 - val_loss: 1.1104 - val_accuracy: 0.8115\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9824 - val_loss: 1.1105 - val_accuracy: 0.8101\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0519 - accuracy: 0.9819 - val_loss: 1.1965 - val_accuracy: 0.8099\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 1.2700 - val_accuracy: 0.8083\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 1.2767 - val_accuracy: 0.8059\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0445 - accuracy: 0.9851 - val_loss: 1.2911 - val_accuracy: 0.8096\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0425 - accuracy: 0.9854 - val_loss: 1.3781 - val_accuracy: 0.8076\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0421 - accuracy: 0.9857 - val_loss: 1.3859 - val_accuracy: 0.8080\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0421 - accuracy: 0.9855 - val_loss: 1.4579 - val_accuracy: 0.8090\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.9854 - val_loss: 1.4280 - val_accuracy: 0.8050\n",
      "1537/1537 - 2s - loss: 0.6935 - accuracy: 0.5027\n",
      "[0.6934597492218018, 0.5027361512184143]\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_RNN.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_model_RNN= model_CNN.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "                           \n",
    "# 테스트셋을 통한 모델 평가\n",
    "results_RNN = model_RNN.evaluate(x_test,  y_test, verbose=2)\n",
    "print(results_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0415 - accuracy: 0.9856 - val_loss: 1.4921 - val_accuracy: 0.8069\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0399 - accuracy: 0.9859 - val_loss: 1.4635 - val_accuracy: 0.8051\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0392 - accuracy: 0.9863 - val_loss: 1.5097 - val_accuracy: 0.7975\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0387 - accuracy: 0.9867 - val_loss: 1.5622 - val_accuracy: 0.8011\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 1.5974 - val_accuracy: 0.8015\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0389 - accuracy: 0.9864 - val_loss: 1.6254 - val_accuracy: 0.8032\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0384 - accuracy: 0.9866 - val_loss: 1.6564 - val_accuracy: 0.7990\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0397 - accuracy: 0.9859 - val_loss: 1.6417 - val_accuracy: 0.8012\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 1.6802 - val_accuracy: 0.8022\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 1s 4ms/step - loss: 0.0361 - accuracy: 0.9872 - val_loss: 1.6738 - val_accuracy: 0.8031\n",
      "1537/1537 - 13s - loss: 0.6930 - accuracy: 0.5112\n",
      "[0.6930289268493652, 0.5111581087112427]\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"compile\"\n",
    "model_LSTM.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_model_LSTM= model_CNN.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "                           \n",
    "# 테스트셋을 통한 모델 평가\n",
    "results_LSTM = model_LSTM.evaluate(x_test,  y_test, verbose=2)\n",
    "print(results_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
