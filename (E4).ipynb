{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화리뷰 감성분석 도전하기\n",
    "\\$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "\\$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "\\$ mv ratings_*.txt ~/aiffel/sentiment_classification\n",
    "\n",
    "$ python3\n",
    "```\n",
    ">>> from konlpy.tag import Komoran\n",
    ">>> komoran = Komoran() \n",
    "```\n",
    "에서 계속 오류가 났지만,  \n",
    "\n",
    "https://zereight.tistory.com/650  \n",
    "덕분에 Komoran을 돌릴 수 있다.\n",
    "\n",
    "https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 데이터로더 구성\n",
    "\n",
    "실습때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.\n",
    "\n",
    "    데이터의 중복 제거\n",
    "    NaN 결측치 제거\n",
    "    한국어 토크나이저로 토큰화\n",
    "    불용어(Stopwords) 제거\n",
    "    사전word_to_index 구성\n",
    "    텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "    X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간오래걸린다\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 어떤 비율로 train과 test를 나눴는지 모르겠음\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "#     index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8410, 158, 3925, 4, 4, 298, 94, 17, 6, 4, 4, 58, 3479]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[3]) # 왜 BOS로 시작 안하지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8410, 158, 3925, 4, 4, 298, 94, 17, 6, 4, 4, 58, 3479]\n",
      "교도소 이야기 구먼 . . 솔직히 재미 없 다 . . 평점 조정\n",
      "라벨:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[:])\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "\n",
    "print(x_train[3])\n",
    "print(get_decoded_sentence(x_train[3], index_to_word))\n",
    "print('라벨: ', y_train[3])  # 6번째 리뷰데이터의 라벨 부정은 0, 긍정은 1\n",
    "\n",
    "# x_train에 아직 BOS 안 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교도소\n",
      "이야기\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[8410])\n",
    "print(index_to_word[158]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146182\n",
      "49157\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3473, 5, 2]\n",
      "[[1, 122], [1, 269], [1, 102]]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('아이언맨 영화 좋다', word_to_index))\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "encoded_sentences = get_encoded_sentences('개노잼', word_to_index) # 이해안감\n",
    "print(encoded_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금 보니까 보정 안하는 게 맞음\n",
    "# # 보정 -> 실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "# word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# # 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "# word_to_index[\"<PAD>\"] = 0\n",
    "# word_to_index[\"<BOS>\"] = 1\n",
    "# word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "# word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# index_to_word[0] = \"<PAD>\"\n",
    "# index_to_word[1] = \"<BOS>\"\n",
    "# index_to_word[2] = \"<UNK>\"\n",
    "# index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "# index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "# print(x_train[3])\n",
    "# print(get_decoded_sentence(x_train[3], index_to_word))\n",
    "# print('라벨: ', y_train[3])  # 6번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\n",
      "<BOS>\n",
      "<UNK>\n",
      "<UNUSED>\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[0]) \n",
    "print(index_to_word[1]) \n",
    "print(index_to_word[2]) \n",
    "print(index_to_word[3])  # padding 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8410, 158, 3925, 4, 4, 298, 94, 17, 6, 4, 4, 58, 3479]\n",
      "교도소 이야기 구먼 . . 솔직히 재미 없 다 . . 평점 조정\n",
      "라벨:  0\n"
     ]
    }
   ],
   "source": [
    "nn = 3\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 118, 10, 7, 2262, 418, 27, 900]\n",
      "나 이거 보 고 인형 절대 안 삼\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "nn = 500\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding 숫자 출력할 때 왜 1이 안나올까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 93.42988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트 데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)*100 / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x_train data 개수, 각 data의 length) = (146182, 41)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # for RNN\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                      value=word_to_index[\"<PAD>\"],\n",
    "                                                     padding='pre', # for RNN\n",
    "                                                      maxlen=maxlen)\n",
    "\n",
    "print('(x_train data 개수, 각 data의 length) =', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " 8410  158 3925    4    4  298   94   17    6    4    4   58 3479]\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> 교도소 이야기 구먼 . . 솔직히 재미 없 다 . . 평점 조정\n",
      "라벨:  0\n"
     ]
    }
   ],
   "source": [
    "nn = 3\n",
    "print(x_train[nn])\n",
    "print(get_decoded_sentence(x_train[nn], index_to_word))\n",
    "print('라벨: ', y_train[nn])  # (nn+1)번째 리뷰데이터의 라벨 부정은 0, 긍정은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - RNN\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,))) # 동일 # model의 첫번째 레이어\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu')) # 동일\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다. # 동일\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116182, 41)\n",
      "(116182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 분리\n",
    "x_val = x_train[:30000]   \n",
    "y_val = y_train[:30000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[30000:]  \n",
    "partial_y_train = y_train[30000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "78/78 [==============================] - 5s 61ms/step - loss: 0.6306 - accuracy: 0.6731 - val_loss: 0.4379 - val_accuracy: 0.8127\n",
      "Epoch 2/20\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.3745 - accuracy: 0.8385 - val_loss: 0.3565 - val_accuracy: 0.8445\n",
      "Epoch 3/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8644 - val_loss: 0.3495 - val_accuracy: 0.8480\n",
      "Epoch 4/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8744 - val_loss: 0.3503 - val_accuracy: 0.8500\n",
      "Epoch 5/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.2863 - accuracy: 0.8834 - val_loss: 0.3519 - val_accuracy: 0.8498\n",
      "Epoch 6/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.2704 - accuracy: 0.8917 - val_loss: 0.3542 - val_accuracy: 0.8484\n",
      "Epoch 7/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.2529 - accuracy: 0.8997 - val_loss: 0.3592 - val_accuracy: 0.8483\n",
      "Epoch 8/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.2340 - accuracy: 0.9092 - val_loss: 0.3675 - val_accuracy: 0.8486\n",
      "Epoch 9/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.2121 - accuracy: 0.9207 - val_loss: 0.3812 - val_accuracy: 0.8477\n",
      "Epoch 10/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1901 - accuracy: 0.9319 - val_loss: 0.3976 - val_accuracy: 0.8455\n",
      "Epoch 11/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9418 - val_loss: 0.4230 - val_accuracy: 0.8438\n",
      "Epoch 12/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1469 - accuracy: 0.9511 - val_loss: 0.4438 - val_accuracy: 0.8423\n",
      "Epoch 13/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1282 - accuracy: 0.9591 - val_loss: 0.4695 - val_accuracy: 0.8390\n",
      "Epoch 14/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9650 - val_loss: 0.4984 - val_accuracy: 0.8386\n",
      "Epoch 15/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0973 - accuracy: 0.9710 - val_loss: 0.5321 - val_accuracy: 0.8342\n",
      "Epoch 16/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.9750 - val_loss: 0.5643 - val_accuracy: 0.8341\n",
      "Epoch 17/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0755 - accuracy: 0.9784 - val_loss: 0.5960 - val_accuracy: 0.8312\n",
      "Epoch 18/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0669 - accuracy: 0.9816 - val_loss: 0.6253 - val_accuracy: 0.8300\n",
      "Epoch 19/20\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9831 - val_loss: 0.6605 - val_accuracy: 0.8275\n",
      "Epoch 20/20\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0543 - accuracy: 0.9849 - val_loss: 0.6979 - val_accuracy: 0.8249\n"
     ]
    }
   ],
   "source": [
    "# model 학습 시작 \"complie\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "# fit에 주목\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1500,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.7252 - accuracy: 0.8191\n",
      "[0.7251625061035156, 0.819069504737854]\n"
     ]
    }
   ],
   "source": [
    "# test 는 \"evaluate\"\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 모델구성을 위한 데이터 분석 및 가공\n",
    "\n",
    "    데이터셋 내 문장 길이 분포\n",
    "    적절한 최대 문장 길이 지정\n",
    "    keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가\n",
    "\n",
    "4) 모델구성 및 validation set 구성\n",
    "\n",
    "모델은 3가지 이상 다양하게 구성하여 실험해 보세요.\n",
    "5) 모델 훈련 개시\n",
    "6) Loss, Accuracy 그래프 시각화\n",
    "7) 학습된 Embedding 레이어 분석\n",
    "8) 한국어 Word2Vec 임베딩 활용하여 성능개선\n",
    "\n",
    "한국어 Word2Vec은 다음 경로에서 구할 수 있습니다.\n",
    "\n",
    "https://github.com/Kyubyong/wordvectors\n",
    "\n",
    "평가문항\t\n",
    " 상세기준\n",
    "\n",
    "1. 다양한 방법으로 Text Classification 태스크를 성공적으로 구현하였다.\n",
    "\t3가지 이상의 모델이 성공적으로 시도됨\n",
    "\n",
    "2. gensim을 활용하여 자체학습된 혹은 사전학습된 임베딩 레이어를 분석하였다.\n",
    "\tgensim의 유사단어 찾기를 활용하여 자체학습한 임베딩과 사전학습 임베딩을 적절히 분석함\n",
    "\n",
    "3. 한국어 Word2Vec을 활용하여 가시적인 성능향상을 달성했다.\n",
    "\t네이버 영화리뷰 데이터 감성분석 정확도를 85% 이상 달성함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
